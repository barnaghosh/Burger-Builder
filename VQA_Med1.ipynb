{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1grZWc2yPwcPGROsnU06qxCxLlheOe_TP",
      "authorship_tag": "ABX9TyMj/ooECvvAdrde79fpIV65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barnaghosh/Burger-Builder/blob/main/VQA_Med1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-3Q7Lso3vEsT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training set\n",
        "train_df = pd.read_json(\"/content/drive/MyDrive/VQA/input/vqa rad/Visual-Question-Answering-master/Visual-Question-Answering-master/trainset.json\")\n",
        "\n",
        "# Load the test set\n",
        "test_df = pd.read_json(\"/content/drive/MyDrive/VQA/input/vqa rad/Visual-Question-Answering-master/Visual-Question-Answering-master/testset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define the image size and the path to the images\n",
        "image_size = (224, 224)\n",
        "image_folder = \"/content/drive/MyDrive/VQA/input/vqa rad/Visual-Question-Answering-master/Visual-Question-Answering-master/VQA_RAD Image Folder\"\n",
        "\n",
        "def load_and_preprocess_image(image_name):\n",
        "    # Open the image file\n",
        "    img = Image.open(f\"{image_folder}/{image_name}\")\n",
        "    # Resize the image\n",
        "    img = img.resize(image_size)\n",
        "    # Convert the image data to a numpy array\n",
        "    image = np.array(img)\n",
        "    # Normalize the image\n",
        "    image = image / 255.0\n",
        "\n",
        "    return image\n",
        "# Load and preprocess the images in the training set\n",
        "train_images = np.array([load_and_preprocess_image(image_name) for image_name in train_df[\"image_name\"]])\n",
        "\n",
        "# Load and preprocess the images in the test set\n",
        "test_images = np.array([load_and_preprocess_image(image_name) for image_name in test_df[\"image_name\"]])"
      ],
      "metadata": {
        "id": "GLMoN19FwUP7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_seq_length = 100\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the questions\n",
        "tokenizer.fit_on_texts(train_df[\"question\"])\n",
        "\n",
        "# Convert the questions to sequences of tokens\n",
        "train_questions = tokenizer.texts_to_sequences(train_df[\"question\"])\n",
        "test_questions = tokenizer.texts_to_sequences(test_df[\"question\"])\n",
        "\n",
        "# Pad the sequences\n",
        "train_questions = pad_sequences(train_questions, maxlen=max_seq_length)\n",
        "test_questions = pad_sequences(test_questions, maxlen=max_seq_length)"
      ],
      "metadata": {
        "id": "2eopVxK4wsi9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Create a binarizer\n",
        "binarizer = LabelBinarizer()\n",
        "\n",
        "# Ensure all answers are strings\n",
        "train_df[\"answer\"] = train_df[\"answer\"].astype(str)\n",
        "test_df[\"answer\"] = test_df[\"answer\"].astype(str)\n",
        "\n",
        "# Fit the binarizer and transform the answers\n",
        "train_answers = binarizer.fit_transform(train_df[\"answer\"])\n",
        "test_answers = binarizer.transform(test_df[\"answer\"])"
      ],
      "metadata": {
        "id": "hHG4KK2ywwU3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, concatenate\n",
        "\n",
        "# Define the image model\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "image_model = VGG16(include_top=False, weights='imagenet', input_tensor=image_input)\n",
        "for layer in image_model.layers:\n",
        "    layer.trainable = False\n",
        "image_model = Flatten()(image_model.output)\n",
        "\n",
        "# Define the question model\n",
        "question_input = Input(shape=(max_seq_length,))\n",
        "question_model = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=256, input_length=max_seq_length)(question_input)\n",
        "question_model = LSTM(256)(question_model)"
      ],
      "metadata": {
        "id": "fVRN-cyJw8GK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the models\n",
        "combined = concatenate([image_model, question_model])\n",
        "\n",
        "# Add the classifier on top\n",
        "output = Dense(len(binarizer.classes_), activation='softmax')(combined)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Ackq2WyKxRIg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Define the image model\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "image_model = VGG16(include_top=False, weights='imagenet', input_tensor=image_input)\n",
        "for layer in image_model.layers:\n",
        "    layer.trainable = False\n",
        "image_model = Flatten()(image_model.output)\n",
        "image_model = Dropout(0.5)(image_model)  # Add dropout layer\n",
        "\n",
        "# Define the question model\n",
        "question_input = Input(shape=(max_seq_length,))\n",
        "question_model = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=256, input_length=max_seq_length)(question_input)\n",
        "question_model = LSTM(256)(question_model)\n",
        "question_model = Dropout(0.5)(question_model)  # Add dropout layer\n",
        "# Define the question model\n",
        "question_input = Input(shape=(max_seq_length,))\n",
        "question_model = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=256, input_length=max_seq_length)(question_input)\n",
        "question_model = LSTM(256)(question_model)\n",
        "question_model = Dropout(0.5)(question_model)  # Add dropout layer\n",
        "\n",
        "# Combine the models\n",
        "combined = concatenate([image_model, question_model])\n",
        "\n",
        "# Add the classifier on top\n",
        "output = Dense(len(binarizer.classes_), activation='softmax')(combined)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LZjnUEwuxwND"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3),\n",
        "    ModelCheckpoint('vqa_model.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [train_images, train_questions],\n",
        "    train_answers,\n",
        "    epochs=30,  # Increased number of epochs\n",
        "    validation_data=([test_images, test_questions], test_answers),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODdYcrHgyAsw",
        "outputId": "5346f8d3-cfd5-4860-c3b4-a71653ceab9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "96/96 [==============================] - 49s 334ms/step - loss: 4.8897 - accuracy: 0.2523 - val_loss: 3.7324 - val_accuracy: 0.2727\n",
            "Epoch 2/30\n",
            "96/96 [==============================] - 18s 186ms/step - loss: 4.1201 - accuracy: 0.3437 - val_loss: 4.2694 - val_accuracy: 0.2860\n",
            "Epoch 3/30\n",
            "96/96 [==============================] - 18s 185ms/step - loss: 3.8186 - accuracy: 0.4253 - val_loss: 4.2673 - val_accuracy: 0.3769\n",
            "Epoch 4/30\n",
            "96/96 [==============================] - 17s 182ms/step - loss: 3.8409 - accuracy: 0.5196 - val_loss: 4.6061 - val_accuracy: 0.4545\n",
            "Epoch 5/30\n",
            "96/96 [==============================] - 17s 179ms/step - loss: 3.6748 - accuracy: 0.5623 - val_loss: 4.7378 - val_accuracy: 0.4368\n",
            "Epoch 6/30\n",
            "96/96 [==============================] - 17s 181ms/step - loss: 3.6590 - accuracy: 0.5760 - val_loss: 4.5618 - val_accuracy: 0.3814\n",
            "Epoch 7/30\n",
            "96/96 [==============================] - 16s 169ms/step - loss: 3.4320 - accuracy: 0.5943 - val_loss: 4.5595 - val_accuracy: 0.4279\n",
            "Epoch 8/30\n",
            "96/96 [==============================] - 17s 172ms/step - loss: 3.2549 - accuracy: 0.6260 - val_loss: 4.0359 - val_accuracy: 0.4745\n",
            "Epoch 9/30\n",
            "96/96 [==============================] - 17s 177ms/step - loss: 2.9548 - accuracy: 0.6426 - val_loss: 4.3098 - val_accuracy: 0.4701\n",
            "Epoch 10/30\n",
            "96/96 [==============================] - 17s 174ms/step - loss: 2.8472 - accuracy: 0.6560 - val_loss: 4.0193 - val_accuracy: 0.4989\n",
            "Epoch 11/30\n",
            "96/96 [==============================] - 17s 175ms/step - loss: 2.4879 - accuracy: 0.6929 - val_loss: 4.1523 - val_accuracy: 0.4878\n",
            "Epoch 12/30\n",
            "96/96 [==============================] - 17s 176ms/step - loss: 2.4252 - accuracy: 0.7069 - val_loss: 4.4764 - val_accuracy: 0.4834\n",
            "Epoch 13/30\n",
            "96/96 [==============================] - 17s 174ms/step - loss: 2.1419 - accuracy: 0.7471 - val_loss: 4.5650 - val_accuracy: 0.5033\n",
            "Epoch 14/30\n",
            "96/96 [==============================] - 16s 172ms/step - loss: 1.9777 - accuracy: 0.7477 - val_loss: 4.5499 - val_accuracy: 0.5078\n",
            "Epoch 15/30\n",
            "96/96 [==============================] - 17s 181ms/step - loss: 1.9975 - accuracy: 0.7422 - val_loss: 5.0057 - val_accuracy: 0.4789\n",
            "Epoch 16/30\n",
            "96/96 [==============================] - 17s 177ms/step - loss: 1.9356 - accuracy: 0.7640 - val_loss: 4.3580 - val_accuracy: 0.5233\n",
            "Epoch 17/30\n",
            "96/96 [==============================] - 17s 178ms/step - loss: 1.7566 - accuracy: 0.7764 - val_loss: 4.4811 - val_accuracy: 0.5410\n",
            "Epoch 18/30\n",
            "96/96 [==============================] - 17s 182ms/step - loss: 1.7363 - accuracy: 0.7928 - val_loss: 4.5382 - val_accuracy: 0.5299\n",
            "Epoch 19/30\n",
            "96/96 [==============================] - 17s 177ms/step - loss: 1.6077 - accuracy: 0.7973 - val_loss: 4.6139 - val_accuracy: 0.5299\n",
            "Epoch 20/30\n",
            "96/96 [==============================] - 17s 173ms/step - loss: 1.4565 - accuracy: 0.8081 - val_loss: 4.5602 - val_accuracy: 0.5033\n",
            "Epoch 21/30\n",
            "96/96 [==============================] - 16s 172ms/step - loss: 1.5280 - accuracy: 0.8078 - val_loss: 4.2687 - val_accuracy: 0.5477\n",
            "Epoch 22/30\n",
            "96/96 [==============================] - 17s 177ms/step - loss: 1.4559 - accuracy: 0.8107 - val_loss: 4.4885 - val_accuracy: 0.5144\n",
            "Epoch 23/30\n",
            "96/96 [==============================] - 16s 170ms/step - loss: 1.3384 - accuracy: 0.8231 - val_loss: 4.2657 - val_accuracy: 0.5521\n",
            "Epoch 24/30\n",
            "96/96 [==============================] - 16s 171ms/step - loss: 1.2677 - accuracy: 0.8378 - val_loss: 4.3743 - val_accuracy: 0.5588\n",
            "Epoch 25/30\n",
            "96/96 [==============================] - 17s 178ms/step - loss: 1.3281 - accuracy: 0.8404 - val_loss: 4.4914 - val_accuracy: 0.5698\n",
            "Epoch 26/30\n",
            "96/96 [==============================] - 17s 180ms/step - loss: 1.0715 - accuracy: 0.8525 - val_loss: 4.9377 - val_accuracy: 0.5366\n",
            "Epoch 27/30\n",
            "96/96 [==============================] - 17s 173ms/step - loss: 1.0572 - accuracy: 0.8580 - val_loss: 4.8344 - val_accuracy: 0.5299\n",
            "Epoch 28/30\n",
            "96/96 [==============================] - 16s 171ms/step - loss: 1.1827 - accuracy: 0.8404 - val_loss: 4.6582 - val_accuracy: 0.5588\n",
            "Epoch 29/30\n",
            "96/96 [==============================] - 16s 172ms/step - loss: 1.1017 - accuracy: 0.8629 - val_loss: 4.4811 - val_accuracy: 0.5698\n",
            "Epoch 30/30\n",
            "96/96 [==============================] - 17s 173ms/step - loss: 1.1397 - accuracy: 0.8574 - val_loss: 4.6712 - val_accuracy: 0.5388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate([test_images, test_questions], test_answers)\n",
        "print(f\"Test loss: {loss}\")\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtnVzfS2yp9i",
        "outputId": "615a3ee5-df16-446a-f728-ee2689f90db7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 2s 129ms/step - loss: 4.6712 - accuracy: 0.5388\n",
            "Test loss: 4.6711745262146\n",
            "Test accuracy: 0.5388026833534241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "model = model([image_input, question_input], [output])\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "D9EC2km59p8w",
        "outputId": "7527da73-5f6a-4437-f1d0-c57fba7faef3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c4d7303c416f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_constant_value\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_evaluate_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Accept 1/0 as valid boolean values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer 'dropout' (type Dropout).\n\nunhashable type: 'list'\n\nCall arguments received by layer 'dropout' (type Dropout):\n  • inputs=tf.Tensor(shape=(None, 25088), dtype=float32)\n  • training=[\"<KerasTensor: shape=(None, 514) dtype=float32 (created by layer 'dense_1')>\"]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "model = load_model('vqa_model.h5')"
      ],
      "metadata": {
        "id": "fIrH2XAnyQkX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the image model\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "image_model = ResNet50(include_top=False, weights='imagenet', input_tensor=image_input)\n",
        "for layer in image_model.layers:\n",
        "    layer.trainable = False\n",
        "image_model = Flatten()(image_model.output)\n",
        "image_model = Dropout(0.5)(image_model)  # Add dropout layer\n",
        "\n",
        "# Define the question model\n",
        "question_input = Input(shape=(max_seq_length,))\n",
        "question_model = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=256, input_length=max_seq_length)(question_input)\n",
        "question_model = LSTM(256)(question_model)\n",
        "question_model = Dropout(0.5)(question_model)  # Add dropout layer\n",
        "\n",
        "# Combine the models\n",
        "combined = concatenate([image_model, question_model])\n",
        "# Add the classifier on top\n",
        "output = Dense(len(binarizer.classes_), activation='softmax')(combined)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3),\n",
        "    ModelCheckpoint('vqa_model.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [train_images, train_questions],\n",
        "    train_answers,\n",
        "    epochs=20,  # Increased number of epochs\n",
        "    validation_data=([test_images, test_questions], test_answers),\n",
        "    callbacks=callbacks  # Added callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "3cuK8alEyUan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate([test_images, test_questions], test_answers)\n",
        "print(f\"Test loss: {loss}\")\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "xGEyUCr64PUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(image, question):\n",
        "    # Preprocess the image\n",
        "    image = load_and_preprocess_image(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Preprocess the question\n",
        "    question = tokenizer.texts_to_sequences([question])\n",
        "    question = pad_sequences(question, maxlen=max_seq_length)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict([image, question])\n",
        "\n",
        "    # Decode the prediction\n",
        "    answer = binarizer.inverse_transform(prediction)\n",
        "\n",
        "    return answer[0]"
      ],
      "metadata": {
        "id": "yK-R7BbwzW7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = \"synpic100132.jpg\"\n",
        "question = \"What is organ is present?\"\n",
        "print(predict(image, question))\n"
      ],
      "metadata": {
        "id": "4TVILIf2zYqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}